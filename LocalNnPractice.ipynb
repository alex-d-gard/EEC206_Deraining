{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LocalNnPractice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzOGAAcFM6TnQV3M2FAGtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-d-gard/EEC206_Deraining/blob/main/LocalNnPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "74xw8bJRfiYO"
      },
      "outputs": [],
      "source": [
        "#########################################################################\n",
        "### Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#########################################################################\n",
        "### Define the Network\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "#########################################################################\n",
        "### Initialize instance of Network and Optimizer\n",
        "\n",
        "net = Net() #Initialize \n",
        "#print(net)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01) #create optimizer\n",
        "\n",
        "# List amount of parameters (optional)\n",
        "params = list(net.parameters())\n",
        "#print(len(params))\n",
        "#print(params[0].size())  # conv1's .weight\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "### Obtain Training Data\n",
        "\n",
        "input = torch.randn(1, 1, 32, 32) # 1 image, 1 channel, 32x32 size\n",
        "#print(input)\n",
        "\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n"
      ],
      "metadata": {
        "id": "0-JnKn_OgDUl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "### Forward Pass Inputs through Network\n",
        "\n",
        "out = net(input) #forward pass input through network\n",
        "#print(out) #View output with existing network (see how derained it is)\n",
        "\n",
        "#########################################################################\n",
        "### Compute Loss, Back Propagate, and Update\n",
        "\n",
        "# Needs a loop appropriately\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(out, target) # Compute Loss\n",
        "#print(loss)\n",
        "\n",
        "optimizer.zero_grad()   # zero the gradient buffers because they don't overwrite, they accumulate (don't want to add gradients ontop of old gradients)\n",
        "\n",
        "loss.backward()  #Compute gradients\n",
        "optimizer.step()    #Implement Update\n",
        "\n",
        "#########################################################################\n",
        "### Rerun Forward Pass\n",
        "\n",
        "out = net(input) #forward pass input through network\n",
        "print(out) #View output with existing network (see how derained it is)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tZZujJ9gUmU",
        "outputId": "d9be7135-d7cb-4836-ab4d-b2e40e27b503"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6236, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    }
  ]
}